{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install xgboost \n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the datasets (datasets were already splited and saved as csv files, 60, 20, 20 split)\n",
    "train_data = pd.read_csv(\"train_Boarderline_smote_B_data.csv\") #train data\n",
    "test_data = pd.read_csv(\"test_B_data.csv\") #test data\n",
    "eval_data = pd.read_csv(\"external_eval_B_data.csv\") #external eval data\n",
    "\n",
    "# Step 2: Separate labels and features\n",
    "y_train, X_train = train_data.iloc[:, 1], train_data.iloc[:, 2:]\n",
    "y_test, X_test = test_data.iloc[:, 1], test_data.iloc[:, 2:]\n",
    "y_eval, X_eval = eval_data.iloc[:, 1], eval_data.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the XGBoost model and hyperparameter grid\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42) #Random state for reproducibility, logloss for binary classification\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], #Number of boosting rounds:Range: 100-1000, Default: 100\n",
    "    'max_depth': [3, 6, 10], #Maximum tree depth:Range: 3-10, Default: 6\n",
    "    'learning_rate': [0.01, 0.1, 0.2], #Step size shrinkage used to prevent ovexgbitting:Range: 0.01-0.3, Default: 0.3\n",
    "    'subsample': [0.6, 0.8, 1.0], #Fraction of samples used for training trees, Range: 0.5-1.0, Default: 1\n",
    "    'min_child_weight': [1], #Minimum sum of instance weight (hessian) needed in a child, Range: 1-10, Default: 1\n",
    "}\n",
    "\n",
    "# Step 4: Apply GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid, #param_grid is defined above\n",
    "    scoring='accuracy', #scoring is set to 'accuracy'\n",
    "    n_jobs= 1, #number of jobs to run in parallel, reasonable value is -1\n",
    "    cv=5,\n",
    "    verbose=2 #verbose means how much information to print, 2 means print everything\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Print the Best Hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Step 6: Use the best model from GridSearchCV\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Save best hyperparameters to a file\n",
    "with open('best_model_info_xgb.txt', 'w') as f:\n",
    "    f.write(\"Best Hyperparameters:\\n\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "print(\"Best model information has been saved to 'best_model_info_xgb.txt'\")\n",
    "\n",
    "#save best model to a file\n",
    "joblib.dump(best_xgb_model, \"best_xgb_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Fit the model on the training data\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Evaluate the model on the test set\n",
    "y_test_pred = best_xgb_model.predict(X_test)\n",
    "y_test_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 10: Evaluate the model on the external validation set\n",
    "y_eval_pred = best_xgb_model.predict(X_eval)\n",
    "y_eval_prob = best_xgb_model.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Step 11: Calculate and print accuracy, classification report, and confusion matrix\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "ext_accuracy = accuracy_score(y_eval, y_eval_pred)\n",
    "\n",
    "print(f\"XGBoost Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"XGBoost External Validation Accuracy: {ext_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 11: Compute confusion matrices\n",
    "confusion_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "confusion_matrix_eval = confusion_matrix(y_eval, y_eval_pred)\n",
    "print(\"Confusion Matrix (Test Data):\\n\", confusion_matrix_test)\n",
    "print(\"Confusion Matrix (Evaluation Data):\\n\", confusion_matrix_eval)\n",
    "\n",
    "# Step 12: Compute specificity and sensitivity for test and evaluation data\n",
    "# Sensitivity (Recall) and Specificity calculation\n",
    "def calculate_sensitivity_specificity(conf_matrix):\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "sensitivity_test, specificity_test = calculate_sensitivity_specificity(confusion_matrix_test)\n",
    "sensitivity_eval, specificity_eval = calculate_sensitivity_specificity(confusion_matrix_eval)\n",
    "print(\"Test Data - Sensitivity:\", sensitivity_test, \"Specificity:\", specificity_test)\n",
    "print(\"Evaluation Data - Sensitivity:\", sensitivity_eval, \"Specificity:\", specificity_eval)\n",
    "\n",
    "# Step 13: ROC and AUC for Test and Evaluation Datasets\n",
    "y_test_prob = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "y_eval_prob = best_xgb_model.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# ROC for test dataset\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test ROC curve (AUC = {roc_auc_test:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Test Data\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# ROC for evaluation dataset\n",
    "fpr_eval, tpr_eval, _ = roc_curve(y_eval, y_eval_prob)\n",
    "roc_auc_eval = auc(fpr_eval, tpr_eval)\n",
    "plt.figure()\n",
    "plt.plot(fpr_eval, tpr_eval, label=f\"Evaluation ROC curve (AUC = {roc_auc_eval:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Evaluation Data\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"xgb_roc_auc_curves.png\", dpi=500)\n",
    "plt.savefig(\"xgb_roc_auc_curves.pdf\")\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_filename = \"best_xgb_model.pkl\"\n",
    "joblib.dump(best_xgb_model, model_filename)\n",
    "print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up XGBoost model with extra weight for Class 1\n",
    "xgb_model_1 = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=16,  # Experiment with values like 1.5, 2.0, etc.8, 16, \n",
    "    max_depth=10,\n",
    "    learning_rate=0.2,\n",
    "    n_estimators=1000,  #200, 500, 1000\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    min_child_weight=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for Class 1\n",
    "y_test_prob = xgb_model_1.predict_proba(X_test)[:, 1]\n",
    "y_eval_prob = xgb_model_1.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Convert probabilities to binary predictions (using default threshold = 0.5)\n",
    "y_test_pred = (y_test_prob >= 0.5).astype(int)\n",
    "y_eval_pred = (y_eval_prob >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Test Metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, pos_label=1)\n",
    "test_recall = recall_score(y_test, y_test_pred, pos_label=1)\n",
    "test_f1 = f1_score(y_test, y_test_pred, pos_label=1)\n",
    "\n",
    "# Validation Metrics\n",
    "val_accuracy = accuracy_score(y_eval, y_eval_pred)\n",
    "val_precision = precision_score(y_eval, y_eval_pred, pos_label=1)\n",
    "val_recall = recall_score(y_eval, y_eval_pred, pos_label=1)\n",
    "val_f1 = f1_score(y_eval, y_eval_pred, pos_label=1)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Set: Accuracy = {test_accuracy:.2f}, Precision = {test_precision:.2f}, Recall = {test_recall:.2f}, F1 Score = {test_f1:.2f}\")\n",
    "print(f\"Validation Set: Accuracy = {val_accuracy:.2f}, Precision = {val_precision:.2f}, Recall = {val_recall:.2f}, F1 Score = {val_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prob = xgb_model_1.predict_proba(X_test)[:, 1]  # Probabilities for Class 1\n",
    "y_eval_prob = xgb_model_1.predict_proba(X_eval)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TUNING THE THRESHHOLD\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_test_prob = xgb_model_1.predict_proba(X_test)[:, 1]  # Probabilities for Class 1\n",
    "y_eval_prob = xgb_model_1.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "    y_test_adjusted = (y_test_prob >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_test_adjusted, pos_label=1)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold}, Best F1 Score: {best_f1:.2f}\")\n",
    "\n",
    "# Use the best threshold\n",
    "y_test_final = (y_test_prob >= best_threshold).astype(int)\n",
    "y_eval_final = (y_eval_prob >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate final metrics\n",
    "test_precision = precision_score(y_test, y_test_final, pos_label=1)\n",
    "test_recall = recall_score(y_test, y_test_final, pos_label=1)\n",
    "test_f1 = f1_score(y_test, y_test_final, pos_label=1)\n",
    "\n",
    "eval_precision = precision_score(y_eval, y_eval_final, pos_label=1)\n",
    "eval_recall = recall_score(y_eval, y_eval_final, pos_label=1)\n",
    "eval_f1 = f1_score(y_eval, y_eval_final, pos_label=1)\n",
    "\n",
    "print(\"\\nTest Set Metrics (Optimized Threshold):\")\n",
    "print(f\"Precision: {test_precision:.2f}, Recall: {test_recall:.2f}, F1 Score: {test_f1:.2f}\")\n",
    "\n",
    "print(\"\\nValidation Set Metrics (Optimized Threshold):\")\n",
    "print(f\"Precision: {val_precision:.2f}, Recall: {val_recall:.2f}, F1 Score: {val_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tuned XGBoost model to a file\n",
    "with open('tuned_xgb_bcell_model.pkl', 'wb') as file:\n",
    "    pickle.dump(xgb_model_1, file)\n",
    "\n",
    "print(\"Tuned model saved as 'tuned_xgb_bcell_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Fit the model on the training data\n",
    "xgb_model_1.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Evaluate the model on the test set\n",
    "y_test_pred = xgb_model_1.predict(X_test)\n",
    "y_test_prob = xgb_model_1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 10: Evaluate the model on the external validation set\n",
    "y_eval_pred = xgb_model_1.predict(X_eval)\n",
    "y_eval_prob = xgb_model_1.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Step 11: Calculate and print accuracy, classification report, and confusion matrix\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "ext_accuracy = accuracy_score(y_eval, y_eval_pred)\n",
    "\n",
    "print(f\"XGBoost Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"XGBoost External Validation Accuracy: {ext_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 11: Compute confusion matrices\n",
    "confusion_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "confusion_matrix_eval = confusion_matrix(y_eval, y_eval_pred)\n",
    "print(\"Confusion Matrix (Test Data):\\n\", confusion_matrix_test)\n",
    "print(\"Confusion Matrix (Evaluation Data):\\n\", confusion_matrix_eval)\n",
    "\n",
    "# Step 12: Compute specificity and sensitivity for test and evaluation data\n",
    "# Sensitivity (Recall) and Specificity calculation\n",
    "def calculate_sensitivity_specificity(conf_matrix):\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "sensitivity_test, specificity_test = calculate_sensitivity_specificity(confusion_matrix_test)\n",
    "sensitivity_eval, specificity_eval = calculate_sensitivity_specificity(confusion_matrix_eval)\n",
    "print(\"Test Data - Sensitivity:\", sensitivity_test, \"Specificity:\", specificity_test)\n",
    "print(\"Evaluation Data - Sensitivity:\", sensitivity_eval, \"Specificity:\", specificity_eval)\n",
    "\n",
    "# Step 13: ROC and AUC for Test and Evaluation Datasets\n",
    "y_test_prob = xgb_model_1.predict_proba(X_test)[:, 1]\n",
    "y_eval_prob = xgb_model_1.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# ROC for test dataset\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "plt.figure()\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test ROC curve (AUC = {roc_auc_test:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Test Data\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# ROC for evaluation dataset\n",
    "fpr_eval, tpr_eval, _ = roc_curve(y_eval, y_eval_prob)\n",
    "roc_auc_eval = auc(fpr_eval, tpr_eval)\n",
    "plt.figure()\n",
    "plt.plot(fpr_eval, tpr_eval, label=f\"Evaluation ROC curve (AUC = {roc_auc_eval:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Evaluation Data\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"tuned_xgb_bcell_roc_auc_curves.png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Generate ROC for test dataset\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_prob)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Generate ROC for evaluation dataset\n",
    "fpr_eval, tpr_eval, thresholds_eval = roc_curve(y_eval, y_eval_prob)\n",
    "roc_auc_eval = auc(fpr_eval, tpr_eval)\n",
    "\n",
    "# Print the values\n",
    "print(\"Test Data ROC AUC:\", roc_auc_test)\n",
    "print(\"Test Data FPR:\", fpr_test)\n",
    "print(\"Test Data TPR:\", tpr_test)\n",
    "print(\"Test Data Thresholds:\", thresholds_test)\n",
    "\n",
    "print(\"Evaluation Data ROC AUC:\", roc_auc_eval)\n",
    "print(\"Evaluation Data FPR:\", fpr_eval)\n",
    "print(\"Evaluation Data TPR:\", tpr_eval)\n",
    "print(\"Evaluation Data Thresholds:\", thresholds_eval)\n",
    "\n",
    "# Create subplots for side-by-side ROC curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Test Data ROC\n",
    "axes[0].plot(fpr_test, tpr_test, label=f\"ROC AUC = {roc_auc_test:.2f}\", color=\"blue\")\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label=\"Random Classifier (AUC = 0.50)\")\n",
    "axes[0].set_title(\"Test Data - ROC Curve\")\n",
    "axes[0].set_xlabel(\"False Positive Rate\")\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Evaluation Data ROC\n",
    "axes[1].plot(fpr_eval, tpr_eval, label=f\"ROC AUC = {roc_auc_eval:.2f}\", color=\"green\")\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label=\"Random Classifier (AUC = 0.50)\")\n",
    "axes[1].set_title(\"External Validation - ROC Curve\")\n",
    "axes[1].set_xlabel(\"False Positive Rate\")\n",
    "axes[1].set_ylabel(\"True Positive Rate\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# Adjust layout and save figure\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"tuned_xgb_bcell_roc_auc_curves_1.png\", dpi=500)\n",
    "#print(\"ROC curves saved as 'xgb_roc_auc_curves.png'.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of classification report for test and evaluation data\n",
    "print(\"Classification Report (Test Data):\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Classification Report (Evaluation Data):\\n\", classification_report(y_eval, y_eval_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# calculation of F1 score\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "f1_eval = f1_score(y_eval, y_eval_pred)\n",
    "print(\"F1 Score (Test Data):\", f1_test)\n",
    "print(\"F1 Score (Evaluation Data):\", f1_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold cross-validation on the best model\n",
    "cross_val_scores = cross_val_score(xgb_model_1, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"10-Fold Cross-Validation Accuracy Scores:\", cross_val_scores)\n",
    "print(\"Mean 10-Fold CV Accuracy:\", cross_val_scores.mean())\n",
    "\n",
    "# save accuracy of each fold to a text file\n",
    "with open('cv_accuracies_xgb_tuned.txt', 'w') as f:\n",
    "    f.write(\"Cross-Validation Accuracies for each fold:\\n\")\n",
    "    for i, score in enumerate(cross_val_scores):\n",
    "        f.write(f\"Fold {i+1} Accuracy: {score:.4f}\\n\")\n",
    "\n",
    "print(\"Accuracy of each fold has been saved to 'cv_accuracies_xgb_bcell_tuned.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make roc curve of all folds with mean auc and mean accuracy printed on the plot\n",
    "mean_accuracy = cross_val_scores.mean() * 100\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.2f}\")\n",
    "# Step 6: Generate ROC Curve\n",
    "mean_auc = roc_auc_score(y_train, xgb_model_1.predict_proba(X_train)[:, 1])\n",
    "print(f\"Mean AUC: {mean_auc:.4f}\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for i, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    xgb_model_1.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    y_prob = xgb_model_1.predict_proba(X_train.iloc[test_idx])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_train.iloc[test_idx], y_prob)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label=f'Fold {i+1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, label=f'Mean ROC (AUC = {mean_auc:.2f})', lw=2)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"cv_roc_curve_xgb_tuned_bcell.png\", dpi=500)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
