{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym\n",
    "pip install stable-baselines3[extra]\n",
    "pip install shimmy>=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "print(f\"Gym version: {gym.__version__}\")\n",
    "import shimmy\n",
    "print(f\"Shimmy version: {shimmy.__version__}\")\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(\"train_Boarderline_smote_B_data.csv\")\n",
    "test_data = pd.read_csv(\"test_B_data.csv\")\n",
    "eval_data = pd.read_csv(\"external_eval_B_data.csv\")\n",
    "\n",
    "# Inspect the datasets\n",
    "print(f\"Train Data Shape: {train_data.shape}\")\n",
    "print(f\"Test Data Shape: {test_data.shape}\")\n",
    "print(f\"Evaluation Data Shape: {eval_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the Epitope Sequences:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Combine all sequences from train, test, and eval for consistent encoding\n",
    "all_sequences = pd.concat([train_data.iloc[:, 0], test_data.iloc[:, 0], eval_data.iloc[:, 0]])\n",
    "\n",
    "# Initialize and fit LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_sequences)\n",
    "\n",
    "# Transform sequences in all datasets\n",
    "X_train_seq = label_encoder.transform(train_data.iloc[:, 0]).reshape(-1, 1)\n",
    "X_test_seq = label_encoder.transform(test_data.iloc[:, 0]).reshape(-1, 1)\n",
    "X_eval_seq = label_encoder.transform(eval_data.iloc[:, 0]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Encoded Sequences with Numeric Features:\n",
    "import numpy as np\n",
    "\n",
    "# Extract numeric features (columns 3 onward)\n",
    "X_train_numeric = train_data.iloc[:, 2:].values\n",
    "X_test_numeric = test_data.iloc[:, 2:].values\n",
    "X_eval_numeric = eval_data.iloc[:, 2:].values\n",
    "\n",
    "# Combine sequence encodings and numeric features\n",
    "X_train = np.hstack((X_train_seq, X_train_numeric))\n",
    "X_test = np.hstack((X_test_seq, X_test_numeric))\n",
    "X_eval = np.hstack((X_eval_seq, X_eval_numeric))\n",
    "\n",
    "# Labels (binary classification)\n",
    "y_train = train_data.iloc[:, 1].values\n",
    "y_test = test_data.iloc[:, 1].values\n",
    "y_eval = eval_data.iloc[:, 1].values\n",
    "\n",
    "print(f\"X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}\")\n",
    "print(f\"X_test Shape: {X_test.shape}, y_test Shape: {y_test.shape}\")\n",
    "print(f\"X_eval Shape: {X_eval.shape}, y_eval Shape: {y_eval.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for Conv1D (samples, timesteps, features)\n",
    "X_train = X_train.reshape(-1, 767, 1)\n",
    "X_test = X_test.reshape(-1, 767, 1)\n",
    "X_eval = X_eval.reshape(-1, 767, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "class EpitopesEnv(Env):\n",
    "    def __init__(self, features, labels):\n",
    "        super().__init__()\n",
    "        self.features = features  # Features of epitopes (X)\n",
    "        self.labels = labels      # Labels of epitopes (y)\n",
    "        self.n_samples = features.shape[0]\n",
    "        self.current_idx = 0      # Pointer to the current sample\n",
    "\n",
    "        # Action space: Classify as 0 (negative) or 1 (positive)\n",
    "        self.action_space = Discrete(2)\n",
    "\n",
    "        # Observation space: Feature vector of epitope\n",
    "        self.observation_space = Box(\n",
    "            low=np.min(features), high=np.max(features), shape=(features.shape[1],), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.seed_val = None  # To store the seed value\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment\n",
    "        self.current_idx = 0\n",
    "        return self.features[self.current_idx].flatten()  # Flatten to 1D\n",
    "\n",
    "    def step(self, action):\n",
    "        # Reward for correct classification\n",
    "        reward = 1 if action == self.labels[self.current_idx] else -1\n",
    "        \n",
    "        # Move to the next sample\n",
    "        self.current_idx += 1\n",
    "        \n",
    "        # Check if the dataset is exhausted\n",
    "        done = self.current_idx >= self.n_samples\n",
    "        \n",
    "        # Next state\n",
    "        next_state = self.features[self.current_idx].flatten() if not done else None\n",
    "        \n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Set the seed for reproducibility.\"\"\"\n",
    "        self.seed_val = seed\n",
    "        np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "# Wrap the environment\n",
    "env = make_vec_env(lambda: EpitopesEnv(X_train, y_train), n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\n",
    "    'MlpPolicy', \n",
    "    env, \n",
    "    verbose=1, \n",
    "    learning_rate=0.0001,  # Fine-grained updates\n",
    "    gamma=0.95,  # Balances immediate vs. future rewards\n",
    "    batch_size=64,  # Stabilizes updates\n",
    "    exploration_fraction=0.2,  # Standard exploration\n",
    "    exploration_final_eps=0.01,  # Reduced exploration at the end\n",
    "    target_update_interval=1000,  # Frequent updates for responsiveness\n",
    "    train_freq=(1, 'step'),  # Update after every step\n",
    "    gradient_steps=1,  # Backpropagation steps per update\n",
    "    policy_kwargs={\"net_arch\": [256, 128, 128]}  # Deeper neural network architecture\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100000)  # Extended training\n",
    "\n",
    "# Evaluate on test data\n",
    "correct_test = 0\n",
    "for i in range(len(X_test)):\n",
    "    obs = X_test[i].flatten()\n",
    "    action, _ = model.predict(obs)\n",
    "    if action == y_test[i]:\n",
    "        correct_test += 1\n",
    "\n",
    "accuracy_test = correct_test / len(X_test)\n",
    "print(f\"Test Accuracy: {accuracy_test * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on validation data\n",
    "correct_eval = 0\n",
    "for i in range(len(X_eval)):\n",
    "    obs = X_eval[i].flatten()\n",
    "    action, _ = model.predict(obs)\n",
    "    if action == y_eval[i]:\n",
    "        correct_eval += 1\n",
    "\n",
    "accuracy_eval = correct_eval / len(X_eval)\n",
    "print(f\"Validation Accuracy: {accuracy_eval * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(\"dqn_epitope_bcell_classifier\")\n",
    "print(\"DQN model saved as 'dqn_epitope_bcell_classifier'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# Generate predictions for test and validation sets\n",
    "y_test_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    obs = X_test[i].flatten()\n",
    "    action, _ = model.predict(obs)\n",
    "    y_test_pred.append(action)\n",
    "\n",
    "y_eval_pred = []\n",
    "for i in range(len(X_eval)):\n",
    "    obs = X_eval[i].flatten()\n",
    "    action, _ = model.predict(obs)\n",
    "    y_eval_pred.append(action)\n",
    "\n",
    "y_test_pred = np.array(y_test_pred)\n",
    "y_eval_pred = np.array(y_eval_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Test Data\n",
    "tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "\n",
    "# Confusion matrix for Validation Data\n",
    "tn_eval, fp_eval, fn_eval, tp_eval = confusion_matrix(y_eval, y_eval_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Scores\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "f1_eval = f1_score(y_eval, y_eval_pred)\n",
    "\n",
    "# Sensitivity (Recall)\n",
    "sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "sensitivity_eval = tp_eval / (tp_eval + fn_eval)\n",
    "\n",
    "# Specificity\n",
    "specificity_test = tn_test / (tn_test + fp_test)\n",
    "specificity_eval = tn_eval / (tn_eval + fp_eval)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test F1 Score: {f1_test:.2f}\")\n",
    "print(f\"External Validation F1 Score: {f1_eval:.2f}\")\n",
    "print(f\"Test Sensitivity: {sensitivity_test:.2f}\")\n",
    "print(f\"Test Specificity: {specificity_test:.2f}\")\n",
    "print(f\"External Validation Sensitivity: {sensitivity_eval:.2f}\")\n",
    "print(f\"External Validation Specificity: {specificity_eval:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification reports\n",
    "print(\"\\nTest Data - Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"\\nValidation Data - Classification Report:\\n\", classification_report(y_eval, y_eval_pred))\n",
    "\n",
    "# Confusion Matrices\n",
    "print(\"\\nTest Data - Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nValidation Data - Confusion Matrix:\\n\", confusion_matrix(y_eval, y_eval_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what q_values contains\n",
    "obs = X_test[0].flatten()\n",
    "q_values = model.predict(obs)\n",
    "print(\"Q-values type:\", type(q_values))\n",
    "print(\"Q-values content:\", q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predictions to binary labels (0 or 1)\n",
    "y_test_pred = [model.predict(X_test[i].flatten())[0] for i in range(len(X_test))]\n",
    "y_eval_pred = [model.predict(X_eval[i].flatten())[0] for i in range(len(X_eval))]\n",
    "\n",
    "# Compute ROC-AUC using binary predictions\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "roc_auc_eval = roc_auc_score(y_eval, y_eval_pred)\n",
    "\n",
    "print(f\"Test ROC-AUC: {roc_auc_test:.2f}\")\n",
    "print(f\"Validation ROC-AUC: {roc_auc_eval:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Extract true positives, false positives, true negatives, and false negatives from the confusion matrices\n",
    "tp_test, fn_test = 49, 8795\n",
    "fp_test, tn_test = 357, 74448\n",
    "\n",
    "tp_eval, fn_eval = 44, 8800\n",
    "fp_eval, tn_eval = 383, 74422\n",
    "\n",
    "# Calculate TPR (Sensitivity) and FPR for test and validation\n",
    "tpr_test = tp_test / (tp_test + fn_test)\n",
    "fpr_test = fp_test / (fp_test + tn_test)\n",
    "\n",
    "tpr_eval = tp_eval / (tp_eval + fn_eval)\n",
    "fpr_eval = fp_eval / (fp_eval + tn_eval)\n",
    "\n",
    "# Simulate ROC Curves\n",
    "fpr_test_sim = [0, fpr_test, 1]\n",
    "tpr_test_sim = [0, tpr_test, 1]\n",
    "\n",
    "fpr_eval_sim = [0, fpr_eval, 1]\n",
    "tpr_eval_sim = [0, tpr_eval, 1]\n",
    "\n",
    "# Compute AUC for test and validation\n",
    "auc_test = auc(fpr_test_sim, tpr_test_sim)\n",
    "auc_eval = auc(fpr_eval_sim, tpr_eval_sim)\n",
    "\n",
    "# Print ROC data for Test\n",
    "print(\"Test Data - ROC Curve Values\")\n",
    "print(\"False Positive Rate (FPR):\", fpr_test_sim)\n",
    "print(\"True Positive Rate (TPR):\", tpr_test_sim)\n",
    "print(\"Thresholds: [1, ~midpoint, 0]\")  # Simulated thresholds for the three points\n",
    "print(f\"ROC AUC: {auc_test:.2f}\\n\")\n",
    "\n",
    "# Print ROC data for Validation\n",
    "print(\"Validation Data - ROC Curve Values\")\n",
    "print(\"False Positive Rate (FPR):\", fpr_eval_sim)\n",
    "print(\"True Positive Rate (TPR):\", tpr_eval_sim)\n",
    "print(\"Thresholds: [1, ~midpoint, 0]\")  # Simulated thresholds for the three points\n",
    "print(f\"ROC AUC: {auc_eval:.2f}\\n\")\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Test ROC Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr_test_sim, tpr_test_sim, color='blue', label=f'ROC AUC = {auc_test:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier (AUC = 0.50)')\n",
    "plt.title('Test Data - ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Validation ROC Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr_eval_sim, tpr_eval_sim, color='green', label=f'ROC AUC = {auc_eval:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier (AUC = 0.50)')\n",
    "plt.title('Validation Data - ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"dqn_roc_auc_curve.png\", dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = DQN.load(\"dqn_epitope_bcell_classifier\")\n",
    "print(\"DQN model loaded.\")\n",
    "\n",
    "# Initialize StratifiedKFold for 10 splits\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Variables to store results\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for i, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    y_true = y_train[test_idx]\n",
    "    y_prob = []\n",
    "\n",
    "    for idx in test_idx:\n",
    "        obs = X_train[idx].reshape(1, -1)  # Reshape to match input format\n",
    "        # Predict the action and Q-values\n",
    "        action, q_values = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Check if q_values is None\n",
    "        if q_values is None:\n",
    "            q_values = np.array([1.0, 0.0])  # Default probabilities for fallback\n",
    "        \n",
    "        # Normalize Q-values using softmax\n",
    "        probabilities = softmax(q_values)\n",
    "        y_prob.append(probabilities[1])  # Probability of class 1\n",
    "\n",
    "    # Compute accuracy\n",
    "    y_pred = [1 if prob > 0.5 else 0 for prob in y_prob]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC\n",
    "for i, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    y_true = y_train[test_idx]\n",
    "    y_prob = []\n",
    "\n",
    "    for idx in test_idx:\n",
    "        obs = X_train[idx].reshape(1, -1)  # Reshape to match input format\n",
    "        # Predict the action and Q-values\n",
    "        action, q_values = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Check if q_values is None\n",
    "        if q_values is None:\n",
    "            q_values = np.array([1.0, 0.0])  # Default probabilities for fallback\n",
    "        \n",
    "        # Normalize Q-values using softmax\n",
    "        probabilities = softmax(q_values)\n",
    "        y_prob.append(probabilities[1])  # Probability of class 1\n",
    "\n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "    # Plot each fold's ROC curve\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label=f'Fold {i+1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Compute mean ROC curve and AUC\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "\n",
    "# Plot mean ROC curve\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', label=f'Mean ROC (AUC = {mean_auc:.2f})', lw=2)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10-Fold Cross-Validation ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save the ROC curve\n",
    "plt.savefig(\"cv_roc_curve_bcell_dqn.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print mean accuracy\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
