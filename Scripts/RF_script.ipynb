{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the datasets (datasets were already splited and saved as csv files, 60, 20, 20 split)\n",
    "train_data = pd.read_csv(\"train_Boarderline_smote_B_data.csv\") #train data\n",
    "test_data = pd.read_csv(\"test_B_data.csv\") #test data\n",
    "eval_data = pd.read_csv(\"external_eval_B_data.csv\") #external eval data\n",
    "\n",
    "# Step 2: Separate labels and features\n",
    "y_train, X_train = train_data.iloc[:, 1], train_data.iloc[:, 2:]\n",
    "y_test, X_test = test_data.iloc[:, 1], test_data.iloc[:, 2:]\n",
    "y_eval, X_eval = eval_data.iloc[:, 1], eval_data.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Random Forest model and hyperparameter grid\n",
    "rf_model = RandomForestClassifier(random_state=42) # Random state for reproducibility to get consistent results\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], # Number of trees in the forest, default = 100, Range = 10-1000\n",
    "    'max_depth': [10, 20, None], # Maximum depth of the tree, default = None, Range = 1-32, None = full tree\n",
    "    'min_samples_split': [2, 5, 10],# Minimum number of samples required to split an internal node, default = 2, Range = 2-20\n",
    "    'min_samples_leaf': [1, 2, 4], # Minimum number of samples required to be at a leaf node, default = 1, Range = 1-20\n",
    "    'max_features': ['auto', 'sqrt', 'log2'], # Number of features to consider when looking for the best split, default = auto, Range = auto/sqrt/log2\n",
    "}\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=1,\n",
    "    cv=5,\n",
    "    verbose=2\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Print best parameters and model\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Save best hyperparameters to a file\n",
    "with open('best_model_info_rf.txt', 'w') as f:\n",
    "    f.write(\"Best Hyperparameters:\\n\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "print(\"Best model information has been saved to 'best_model_info_rf_bcell.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best hyperparameters to a file\n",
    "with open('best_model_info_rf_bcell.txt', 'w') as f:\n",
    "    f.write(\"Best Hyperparameters:\\n\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "print(\"Best model information has been saved to 'best_model_info_rf_bcell.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "joblib.dump(best_rf_model, \"best_rf_bcell_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Fit the model on the training data\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test and evaluation datasets\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "y_test_prob = best_rf_model.predict_proba(X_test)[:, 1]\n",
    "y_eval_pred = best_rf_model.predict(X_eval)\n",
    "y_eval_prob = best_rf_model.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Print accuracies\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred) * 100:.2f}%\")\n",
    "print(f\"Evaluation Accuracy: {accuracy_score(y_eval, y_eval_pred) * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrices\n",
    "confusion_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "confusion_matrix_eval = confusion_matrix(y_eval, y_eval_pred)\n",
    "print(\"Confusion Matrix (Test Data):\\n\", confusion_matrix_test)\n",
    "print(\"Confusion Matrix (Evaluation Data):\\n\", confusion_matrix_eval)\n",
    "\n",
    "# Sensitivity and Specificity calculation\n",
    "def calculate_sensitivity_specificity(conf_matrix):\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "sensitivity_test, specificity_test = calculate_sensitivity_specificity(confusion_matrix_test)\n",
    "sensitivity_eval, specificity_eval = calculate_sensitivity_specificity(confusion_matrix_eval)\n",
    "print(\"Test Data - Sensitivity:\", sensitivity_test, \"Specificity:\", specificity_test)\n",
    "print(\"Evaluation Data - Sensitivity:\", sensitivity_eval, \"Specificity:\", specificity_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Generate ROC for test dataset\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_prob)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Generate ROC for evaluation dataset\n",
    "fpr_eval, tpr_eval, thresholds_eval = roc_curve(y_eval, y_eval_prob)\n",
    "roc_auc_eval = auc(fpr_eval, tpr_eval)\n",
    "\n",
    "# Print the values\n",
    "print(\"Test Data ROC AUC:\", roc_auc_test)\n",
    "print(\"Test Data FPR:\", fpr_test)\n",
    "print(\"Test Data TPR:\", tpr_test)\n",
    "print(\"Test Data Thresholds:\", thresholds_test)\n",
    "\n",
    "print(\"Evaluation Data ROC AUC:\", roc_auc_eval)\n",
    "print(\"Evaluation Data FPR:\", fpr_eval)\n",
    "print(\"Evaluation Data TPR:\", tpr_eval)\n",
    "print(\"Evaluation Data Thresholds:\", thresholds_eval)\n",
    "\n",
    "# Create subplots for side-by-side ROC curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Test Data ROC\n",
    "axes[0].plot(fpr_test, tpr_test, label=f\"ROC AUC = {roc_auc_test:.2f}\", color=\"blue\")\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label=\"Random Classifier (AUC = 0.50)\")\n",
    "axes[0].set_title(\"Test Data - ROC Curve\")\n",
    "axes[0].set_xlabel(\"False Positive Rate\")\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Evaluation Data ROC\n",
    "axes[1].plot(fpr_eval, tpr_eval, label=f\"ROC AUC = {roc_auc_eval:.2f}\", color=\"green\")\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label=\"Random Classifier (AUC = 0.50)\")\n",
    "axes[1].set_title(\"External Validation - ROC Curve\")\n",
    "axes[1].set_xlabel(\"False Positive Rate\")\n",
    "axes[1].set_ylabel(\"True Positive Rate\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# Adjust layout and save figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rf_bcell_roc_auc_curves.png\", dpi=500)\n",
    "print(\"ROC curves saved as 'rf_roc_auc_curves.png'.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# calculation of F1 score\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "f1_eval = f1_score(y_eval, y_eval_pred)\n",
    "print(\"F1 Score (Test Data):\", f1_test)\n",
    "print(\"F1 Score (Evaluation Data):\", f1_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of classification report for test and evaluation data\n",
    "print(\"Classification Report (Test Data):\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Classification Report (Evaluation Data):\\n\", classification_report(y_eval, y_eval_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold cross-validation on the best model\n",
    "cross_val_scores = cross_val_score(best_rf_model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"10-Fold Cross-Validation Accuracy Scores:\", cross_val_scores)\n",
    "print(\"Mean 10-Fold CV Accuracy:\", cross_val_scores.mean())\n",
    "\n",
    "# save accuracy of each fold to a text file\n",
    "with open('cv_accuracies_bcell_rf.txt', 'w') as f:\n",
    "    f.write(\"Cross-Validation Accuracies for each fold:\\n\")\n",
    "    for i, score in enumerate(cross_val_scores):\n",
    "        f.write(f\"Fold {i+1} Accuracy: {score:.4f}\\n\")\n",
    "\n",
    "print(\"Accuracy of each fold has been saved to 'cv_accuracies_bcell_rf.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make roc curve of all folds with mean auc and mean accuracy printed on the plot\n",
    "mean_accuracy = cross_val_scores.mean() * 100\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.2f}\")\n",
    "# Step 6: Generate ROC Curve\n",
    "mean_auc = roc_auc_score(y_train, best_rf_model.predict_proba(X_train)[:, 1])\n",
    "print(f\"Mean AUC: {mean_auc:.4f}\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for i, (train_idx, test_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    best_rf_model.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    y_prob = best_rf_model.predict_proba(X_train.iloc[test_idx])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_train.iloc[test_idx], y_prob)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label=f'Fold {i+1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, label=f'Mean ROC (AUC = {mean_auc:.2f})', lw=2)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"cv_roc_curve_bcell_rf.png\", dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
