{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user scikeras==0.13.0\n",
    "import sklearn\n",
    "import scikeras\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "print(\"scikeras version:\", scikeras.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the training, test, and external validation datasets\n",
    "train_data = pd.read_csv(\"train_Boarderline_smote_B_data.csv\")\n",
    "test_data = pd.read_csv(\"test_B_data.csv\")\n",
    "eval_data = pd.read_csv(\"external_eval_B_data.csv\")\n",
    "\n",
    "# Step 2: Separate labels and features\n",
    "y_train, X_train = train_data.iloc[:, 1], train_data.iloc[:, 2:]\n",
    "y_test, X_test = test_data.iloc[:, 1], test_data.iloc[:, 2:]\n",
    "y_eval, X_eval = eval_data.iloc[:, 1], eval_data.iloc[:, 2:]\n",
    "\n",
    "# Convert labels to categorical (binary classification)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_eval = to_categorical(y_eval)\n",
    "\n",
    "# Step 3: Define the Neural Network\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')  # 2 nodes for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Step 4: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the model on test and external validation data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "eval_loss, eval_accuracy = model.evaluate(X_eval, y_eval, verbose=0)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"External Validation Accuracy: {eval_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 6: Classification Reports and Confusion Matrices\n",
    "y_test_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_eval_pred = np.argmax(model.predict(X_eval), axis=1)\n",
    "\n",
    "print(\"\\nTest Data - Classification Report:\\n\", classification_report(np.argmax(y_test, axis=1), y_test_pred))\n",
    "print(\"\\nExternal Validation - Classification Report:\\n\", classification_report(np.argmax(y_eval, axis=1), y_eval_pred))\n",
    "\n",
    "print(\"\\nTest Data - Confusion Matrix:\\n\", confusion_matrix(np.argmax(y_test, axis=1), y_test_pred))\n",
    "print(\"\\nExternal Validation - Confusion Matrix:\\n\", confusion_matrix(np.argmax(y_eval, axis=1), y_eval_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: ROC Curves\n",
    "y_test_prob = model.predict(X_test)[:, 1]\n",
    "y_eval_prob = model.predict(X_eval)[:, 1]\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(np.argmax(y_test, axis=1), y_test_prob)\n",
    "fpr_eval, tpr_eval, _ = roc_curve(np.argmax(y_eval, axis=1), y_eval_prob)\n",
    "\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "roc_auc_eval = auc(fpr_eval, tpr_eval)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Test ROC-AUC plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr_test, tpr_test, color='blue', lw=2, label=f'ROC AUC = {roc_auc_test:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('Test Data - ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# External Validation ROC-AUC plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fpr_eval, tpr_eval, color='green', lw=2, label=f'ROC AUC = {roc_auc_eval:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.title('External Validation - ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"nn_roc_auc_bcell_curves.png\", dpi=500)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Save the model\n",
    "model.save(\"nn_epitope_bcell_classifier.h5\")\n",
    "print(\"Model saved as 'nn_epitope_bcell_classifier.h5'\")\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "# Save the model using joblib\n",
    "model_file = \"nn_epitope_bcell_classifier.pkl\"\n",
    "\n",
    "# Save the model's architecture and weights as a tuple\n",
    "dump((model.to_json(), model.get_weights()), model_file)\n",
    "\n",
    "print(f\"Model saved as '{model_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Calculate probabilities for ROC curves\n",
    "y_test_prob = model.predict(X_test)[:, 1]\n",
    "y_eval_prob = model.predict(X_eval)[:, 1]\n",
    "\n",
    "# Calculate FPR, TPR, and thresholds for test and validation sets\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(np.argmax(y_test, axis=1), y_test_prob)\n",
    "fpr_eval, tpr_eval, thresholds_eval = roc_curve(np.argmax(y_eval, axis=1), y_eval_prob)\n",
    "\n",
    "# Calculate ROC AUC values\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "roc_auc_eval = auc(fpr_eval, tpr_eval)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Test Data Metrics:\")\n",
    "print(f\"True Positive Rate (TPR): {tpr_test}\")\n",
    "print(f\"False Positive Rate (FPR): {fpr_test}\")\n",
    "print(f\"Thresholds: {thresholds_test}\")\n",
    "print(f\"ROC AUC: {roc_auc_test:.2f}\")\n",
    "\n",
    "print(\"\\nExternal Validation Data Metrics:\")\n",
    "print(f\"True Positive Rate (TPR): {tpr_eval}\")\n",
    "print(f\"False Positive Rate (FPR): {fpr_eval}\")\n",
    "print(f\"Thresholds: {thresholds_eval}\")\n",
    "print(f\"ROC AUC: {roc_auc_eval:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded true labels to single class labels\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_eval_labels = np.argmax(y_eval, axis=1)\n",
    "y_test_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_eval_pred = np.argmax(model.predict(X_eval), axis=1)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix for Test Data\n",
    "cm_test = confusion_matrix(y_test_labels, y_test_pred)\n",
    "print(\"Confusion Matrix (Test Data):\\n\", cm_test)\n",
    "\n",
    "# Confusion Matrix for External Validation Data\n",
    "cm_eval = confusion_matrix(y_eval_labels, y_eval_pred)\n",
    "print(\"Confusion Matrix (External Validation Data):\\n\", cm_eval)\n",
    "\n",
    "# For Test Data\n",
    "if cm_test.shape == (2, 2):  # Ensure it's binary classification\n",
    "    tn_test, fp_test, fn_test, tp_test = cm_test.ravel()\n",
    "\n",
    "    # Sensitivity (Recall)\n",
    "    test_sensitivity = tp_test / (tp_test + fn_test)\n",
    "\n",
    "    # Specificity\n",
    "    test_specificity = tn_test / (tn_test + fp_test)\n",
    "\n",
    "    print(f\"Test Sensitivity: {test_sensitivity:.2f}\")\n",
    "    print(f\"Test Specificity: {test_specificity:.2f}\")\n",
    "else:\n",
    "    print(\"Confusion matrix is not binary. Cannot compute sensitivity/specificity.\")\n",
    "\n",
    "# For External Validation Data\n",
    "if cm_eval.shape == (2, 2):  # Ensure it's binary classification\n",
    "    tn_eval, fp_eval, fn_eval, tp_eval = cm_eval.ravel()\n",
    "\n",
    "    # Sensitivity (Recall)\n",
    "    eval_sensitivity = tp_eval / (tp_eval + fn_eval)\n",
    "\n",
    "    # Specificity\n",
    "    eval_specificity = tn_eval / (tn_eval + fp_eval)\n",
    "\n",
    "    print(f\"External Validation Sensitivity: {eval_sensitivity:.2f}\")\n",
    "    print(f\"External Validation Specificity: {eval_specificity:.2f}\")\n",
    "else:\n",
    "    print(\"Confusion matrix is not binary. Cannot compute sensitivity/specificity.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model(\"nn_epitope_bcell_classifier.h5\")\n",
    "\n",
    "# Recompile the model with a fresh optimizer\n",
    "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define StratifiedKFold for 10-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Variables to store fold metrics\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "accuracies = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(X_train, np.argmax(y_train, axis=1))):\n",
    "    # Split data for this fold\n",
    "    X_fold_train, X_fold_test = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "    y_fold_train, y_fold_test = y_train[train_idx], y_train[test_idx]\n",
    "    \n",
    "    # Fit the model\n",
    "    loaded_model.fit(X_fold_train, y_fold_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_prob = loaded_model.predict(X_fold_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve and AUC for this fold\n",
    "    fpr, tpr, _ = roc_curve(np.argmax(y_fold_test, axis=1), y_prob)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    \n",
    "    # Plot this fold's ROC curve\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3, label=f'Fold {i+1} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    # Calculate accuracy for this fold\n",
    "    accuracy = loaded_model.evaluate(X_fold_test, y_fold_test, verbose=0)[1]\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plot mean ROC curve\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue', lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the ROC curve plot\n",
    "plt.savefig(\"cv_roc_curve_bcell_nn.png\", dpi=500)\n",
    "plt.show()\n",
    "\n",
    "# Save accuracies and mean accuracy to a text file\n",
    "with open('cv_accuracies_bcell_nn.txt', 'w') as f:\n",
    "    f.write(\"Cross-Validation Accuracies for each fold:\\n\")\n",
    "    for i, accuracy in enumerate(accuracies):\n",
    "        f.write(f\"Fold {i+1} Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"\\nMean 10-Fold CV Accuracy: {np.mean(accuracies):.4f}\\n\")\n",
    "    f.write(f\"Standard Deviation of Accuracy: {np.std(accuracies):.4f}\\n\")\n",
    "\n",
    "print(\"Accuracies and ROC curve saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After computing accuracies for each fold\n",
    "print(\"Cross-Validation Accuracies for each fold:\")\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    print(f\"Fold {i+1} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "# Print mean accuracy and standard deviation\n",
    "print(f\"\\nMean 10-Fold CV Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Standard Deviation of Accuracy: {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded true labels to single class labels\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_eval_labels = np.argmax(y_eval, axis=1)\n",
    "\n",
    "y_test_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_eval_pred = np.argmax(model.predict(X_eval), axis=1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix for Test Data\n",
    "cm_test = confusion_matrix(y_test_labels, y_test_pred)\n",
    "print(\"Confusion Matrix (Test Data):\\n\", cm_test)\n",
    "\n",
    "# Confusion Matrix for External Validation Data\n",
    "cm_eval = confusion_matrix(y_eval_labels, y_eval_pred)\n",
    "print(\"Confusion Matrix (External Validation Data):\\n\", cm_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Test Data\n",
    "if cm_test.shape == (2, 2):  # Ensure it's binary classification\n",
    "    tn_test, fp_test, fn_test, tp_test = cm_test.ravel()\n",
    "\n",
    "    # Sensitivity (Recall)\n",
    "    test_sensitivity = tp_test / (tp_test + fn_test)\n",
    "\n",
    "    # Specificity\n",
    "    test_specificity = tn_test / (tn_test + fp_test)\n",
    "\n",
    "    print(f\"Test Sensitivity: {test_sensitivity:.2f}\")\n",
    "    print(f\"Test Specificity: {test_specificity:.2f}\")\n",
    "else:\n",
    "    print(\"Confusion matrix is not binary. Cannot compute sensitivity/specificity.\")\n",
    "\n",
    "# For External Validation Data\n",
    "if cm_eval.shape == (2, 2):  # Ensure it's binary classification\n",
    "    tn_eval, fp_eval, fn_eval, tp_eval = cm_eval.ravel()\n",
    "\n",
    "    # Sensitivity (Recall)\n",
    "    eval_sensitivity = tp_eval / (tp_eval + fn_eval)\n",
    "\n",
    "    # Specificity\n",
    "    eval_specificity = tn_eval / (tn_eval + fp_eval)\n",
    "\n",
    "    print(f\"External Validation Sensitivity: {eval_sensitivity:.2f}\")\n",
    "    print(f\"External Validation Specificity: {eval_specificity:.2f}\")\n",
    "else:\n",
    "    print(\"Confusion matrix is not binary. Cannot compute sensitivity/specificity.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
