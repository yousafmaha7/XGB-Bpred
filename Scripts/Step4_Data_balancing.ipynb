{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "df_clean = pd.read_excel('length_filtered_epitopes.xlsx')\n",
    "df_minmax = pd.read_csv('minmax_scaled_4lac_data.csv')\n",
    "\n",
    "# Get unique epitopes from both datasets and convert to uppercase\n",
    "clean_epitopes = set(df_clean['Epitope Name'].str.upper())\n",
    "minmax_epitopes = set(df_minmax['epitope'].str.upper())\n",
    "\n",
    "# Find common epitopes\n",
    "common_epitopes = clean_epitopes.intersection(minmax_epitopes)\n",
    "\n",
    "# Filter df_minmax to keep only common epitopes\n",
    "df_common_minmax = df_minmax[df_minmax['epitope'].str.upper().isin(common_epitopes)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "df_common_minmax.to_csv('common_B_epitopes_data.csv', index=False)\n",
    "\n",
    "print(\"Data for common epitopes saved to common_B_epitopes_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the filtered CSV file\n",
    "filtered_file_path = 'common_B_epitopes_data.csv'  \n",
    "df = pd.read_csv(filtered_file_path)\n",
    "\n",
    "# Count positive and negative values in the 'Label' column\n",
    "positive_count = (df['Label'] == 1).sum()\n",
    "negative_count = (df['Label'] == 0).sum()\n",
    "\n",
    "print(f\"Number of Positive values (1): {positive_count}\")\n",
    "print(f\"Number of Negative values (0): {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_csv('common_B_epitopes_data.csv')\n",
    "\n",
    "# Count the occurrences of each class in 'Assay Qualitative Measure'\n",
    "class_counts = df['Label'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Epitope Classes')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.show()\n",
    "\n",
    "# Print the class counts\n",
    "print(\"\\\n",
    "Class Counts:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEFORE APPLYING SMOTE, SPLITTING OF DATA IS CRUCIAL SO THAT EXTERNAL AND TEST DATA REMAINS REAL. SMOTE WILL ONLY BE APPLIED ON TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv(\"common_B_epitopes_data.csv\")  # Replace with actual path\n",
    "\n",
    "# Step 2: Separate the epitope column, features, and labels\n",
    "epitopes = data.iloc[:, 0]  # First column contains epitope IDs\n",
    "X = data.iloc[:, 2:]         # All columns from the 3rd onward are features\n",
    "y = data.iloc[:, 1]          # Second column is the label\n",
    "\n",
    "# Step 3: Perform the first split - 80% (train + test) and 20% (external evaluation)\n",
    "X_temp, X_eval, y_temp, y_eval, epitopes_temp, epitopes_eval = train_test_split(\n",
    "    X, y, epitopes, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Perform the second split - 60% train and 20% test from the 80% temporary data\n",
    "X_train, X_test, y_train, y_test, epitopes_train, epitopes_test = train_test_split(\n",
    "    X_temp, y_temp, epitopes_temp, test_size=0.25, stratify=y_temp, random_state=42  # 0.25 * 80% = 20%\n",
    ")\n",
    "\n",
    "# Step 5: Save the datasets to CSV files, including the epitope column\n",
    "train_data = pd.concat([epitopes_train.reset_index(drop=True), y_train.reset_index(drop=True), X_train.reset_index(drop=True)], axis=1)\n",
    "test_data = pd.concat([epitopes_test.reset_index(drop=True), y_test.reset_index(drop=True), X_test.reset_index(drop=True)], axis=1)\n",
    "eval_data = pd.concat([epitopes_eval.reset_index(drop=True), y_eval.reset_index(drop=True), X_eval.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Step 6: Save to CSV\n",
    "train_data.to_csv(\"train_B_data.csv\", index=False)\n",
    "test_data.to_csv(\"test_B_data.csv\", index=False)\n",
    "eval_data.to_csv(\"external_eval_B_data.csv\", index=False)\n",
    "\n",
    "# Verify sizes\n",
    "print(f\"Training set saved with size: {train_data.shape}\")\n",
    "print(f\"Test set saved with size: {test_data.shape}\")\n",
    "print(f\"External evaluation set saved with size: {eval_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##APPLICATION OF SMOTE ON TRAINING DATA\n",
    "!pip install --upgrade imbalanced-learn\n",
    "import imblearn\n",
    "print(imblearn.__version__)\n",
    "# Check the installed version of scikit-learn\n",
    "import sklearn\n",
    "print(\"Installed scikit-learn version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the training data\n",
    "train_data = pd.read_csv(\"train_B_data.csv\")\n",
    "\n",
    "# Step 2: Separate epitope sequences, features, and labels\n",
    "epitopes_train = train_data.iloc[:, 0]  # Epitope sequences/IDs\n",
    "y_train = train_data.iloc[:, 1]         # Label column\n",
    "X_train = train_data.iloc[:, 2:]        # Feature columns\n",
    "\n",
    "# Step 3: Apply Borderline-SMOTE (without epitopes)\n",
    "smote = BorderlineSMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 4: Handle epitope IDs for synthetic samples\n",
    "# Original IDs (before SMOTE)\n",
    "epitopes_smote = epitopes_train.reset_index(drop=True)\n",
    "\n",
    "# Calculate number of synthetic samples generated\n",
    "num_synthetic = len(X_train_smote) - len(X_train)\n",
    "\n",
    "# Create placeholder epitope IDs for synthetic samples\n",
    "synthetic_ids = [f\"synthetic_epitope_{i+1}\" for i in range(num_synthetic)]\n",
    "\n",
    "# Concatenate original and synthetic IDs\n",
    "all_epitopes = pd.concat([epitopes_smote, pd.Series(synthetic_ids)], ignore_index=True)\n",
    "\n",
    "# Step 5: Combine all data into a single DataFrame\n",
    "train_smote_data = pd.concat([\n",
    "    all_epitopes,                        # Epitope sequences/IDs\n",
    "    pd.Series(y_train_smote).reset_index(drop=True),  # Labels\n",
    "    pd.DataFrame(X_train_smote).reset_index(drop=True)  # Features\n",
    "], axis=1)\n",
    "\n",
    "# Step 6: Save the SMOTE-augmented training data\n",
    "train_smote_data.to_csv(\"train_Boarderline_smote_B_data.csv\", index=False, header=True)\n",
    "\n",
    "# Verify size of the SMOTE-augmented data\n",
    "print(f\"SMOTE-augmented training data saved with size: {train_smote_data.shape}\") "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
