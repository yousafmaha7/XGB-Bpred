{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost Model Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#importing librariers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the datasets (datasets were already splited and saved as csv files, 60, 20, 20 split)\n",
    "train_data = pd.read_csv(\"train_Boarderline_smote_B_data.csv\") #train data\n",
    "test_data = pd.read_csv(\"test_B_data.csv\") #test data\n",
    "eval_data = pd.read_csv(\"external_eval_B_data.csv\") #external eval data\n",
    "\n",
    "# Step 2: Separate labels and features\n",
    "y_train, X_train = train_data.iloc[:, 1], train_data.iloc[:, 2:]\n",
    "y_test, X_test = test_data.iloc[:, 1], test_data.iloc[:, 2:]\n",
    "y_eval, X_eval = eval_data.iloc[:, 1], eval_data.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AdaBoost model and hyperparameter grid\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "adaboost_model = AdaBoostClassifier(estimator=base_estimator, random_state=42) #base_estimator=base_estimator means use the DecisionTreeClassifier as the base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],               # Number of weak learners, default is 50, range is 10 to 1000\n",
    "    'learning_rate': [0.01, 0.1, 1.0],            # Controls contribution of each weak learner, default is 1.0, range is 0.01 to 1.0\n",
    "    'algorithm': ['SAMME.R'],            # Adaptive boosting algorithm, default is SAMME.R, range is SAMME and SAMME.R; SAMME is equivalent to AdaBoost, SAMME.R is generally faster and often performs better for classification tasks.\n",
    "    'estimator__max_depth': [1, 2, 3],       # Depth of the decision tree base estimator, default is 1, range is 1 to 10\n",
    "    'estimator__max_features': ['sqrt', 'log2'], # Number of features to consider when looking for the best split\n",
    "}\n",
    "# GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=adaboost_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=1,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Save best hyperparameters to a file\n",
    "with open('best_model_info_adaboost_bcell.txt', 'w') as f:\n",
    "    f.write(\"Best Hyperparameters:\\n\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "\n",
    "print(\"Best model information has been saved to 'best_model_info_adaboost_bcell.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Use the best model from GridSearchCV\n",
    "best_adaboost_model = grid_search.best_estimator_\n",
    "\n",
    "#save best model to a file\n",
    "joblib.dump(best_adaboost_model, \"best_adaboost_bcell_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Fit the model on the training data\n",
    "best_adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "y_test_pred = best_adaboost_model.predict(X_test)\n",
    "y_test_prob = best_adaboost_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate on the evaluation dataset\n",
    "y_eval_pred = best_adaboost_model.predict(X_eval)\n",
    "y_eval_prob = best_adaboost_model.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Calculate and print accuracy, classification reports, and confusion matrices\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "eval_accuracy = accuracy_score(y_eval, y_eval_pred)\n",
    "print(f\"AdaBoost Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"AdaBoost Evaluation Accuracy: {eval_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrices\n",
    "confusion_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "confusion_matrix_eval = confusion_matrix(y_eval, y_eval_pred)\n",
    "print(\"Confusion Matrix (Test Data):\\n\", confusion_matrix_test)\n",
    "print(\"Confusion Matrix (Evaluation Data):\\n\", confusion_matrix_eval)\n",
    "\n",
    "# Sensitivity and Specificity calculation\n",
    "def calculate_sensitivity_specificity(conf_matrix):\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "sensitivity_test, specificity_test = calculate_sensitivity_specificity(confusion_matrix_test)\n",
    "sensitivity_eval, specificity_eval = calculate_sensitivity_specificity(confusion_matrix_eval)\n",
    "print(\"Test Data - Sensitivity:\", sensitivity_test, \"Specificity:\", specificity_test)\n",
    "print(\"Evaluation Data - Sensitivity:\", sensitivity_eval, \"Specificity:\", specificity_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Generate ROC for test dataset\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_prob)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Generate ROC for evaluation dataset\n",
    "fpr_eval, tpr_eval, thresholds_eval = roc_curve(y_eval, y_eval_prob)\n",
    "roc_auc_eval = auc(fpr_eval, tpr_eval)\n",
    "\n",
    "# Print the values\n",
    "print(\"Test Data ROC AUC:\", roc_auc_test)\n",
    "print(\"Test Data FPR:\", fpr_test)\n",
    "print(\"Test Data TPR:\", tpr_test)\n",
    "print(\"Test Data Thresholds:\", thresholds_test)\n",
    "\n",
    "print(\"Evaluation Data ROC AUC:\", roc_auc_eval)\n",
    "print(\"Evaluation Data FPR:\", fpr_eval)\n",
    "print(\"Evaluation Data TPR:\", tpr_eval)\n",
    "print(\"Evaluation Data Thresholds:\", thresholds_eval)\n",
    "\n",
    "# Create subplots for side-by-side ROC curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Test Data ROC\n",
    "axes[0].plot(fpr_test, tpr_test, label=f\"ROC AUC = {roc_auc_test:.2f}\", color=\"blue\")\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label=\"Random Classifier (AUC = 0.50)\")\n",
    "axes[0].set_title(\"Test Data - ROC Curve\")\n",
    "axes[0].set_xlabel(\"False Positive Rate\")\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Evaluation Data ROC\n",
    "axes[1].plot(fpr_eval, tpr_eval, label=f\"ROC AUC = {roc_auc_eval:.2f}\", color=\"green\")\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label=\"Random Classifier (AUC = 0.50)\")\n",
    "axes[1].set_title(\"External Validation - ROC Curve\")\n",
    "axes[1].set_xlabel(\"False Positive Rate\")\n",
    "axes[1].set_ylabel(\"True Positive Rate\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "# Adjust layout and save figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"adaboost_bcell_roc_auc_curves.png\", dpi=500)\n",
    "print(\"ROC curves saved as 'adaboost_bcell_roc_auc_curves.png'.\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# calculation of F1 score\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "f1_eval = f1_score(y_eval, y_eval_pred)\n",
    "print(\"F1 Score (Test Data):\", f1_test)\n",
    "print(\"F1 Score (Evaluation Data):\", f1_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of classification report for test and evaluation data\n",
    "print(\"Classification Report (Test Data):\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Classification Report (Evaluation Data):\\n\", classification_report(y_eval, y_eval_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold cross-validation on the best model\n",
    "cross_val_scores = cross_val_score(best_adaboost_model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "print(\"10-Fold Cross-Validation Accuracy Scores:\", cross_val_scores)\n",
    "print(\"Mean 10-Fold CV Accuracy:\", cross_val_scores.mean())\n",
    "\n",
    "# save accuracy of each fold to a text file\n",
    "with open('cv_accuracies_bcell_adaboost.txt', 'w') as f:\n",
    "    f.write(\"Cross-Validation Accuracies for each fold:\\n\")\n",
    "    for i, score in enumerate(cross_val_scores):\n",
    "        f.write(f\"Fold {i+1} Accuracy: {score:.4f}\\n\")\n",
    "\n",
    "print(\"Accuracy of each fold has been saved to 'cv_accuracies_bcell_adaboost.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
